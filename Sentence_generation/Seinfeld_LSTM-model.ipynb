{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import keras from tensorflow\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import math\n",
    "import itertools\n",
    "import numpy as np\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_data = pd.read_csv('Sentence_generation/scripts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "seinfeld_text_data = seinfeld_data[['Dialogue']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vocab infor\n",
    "VOCAB_SIZE = 10000\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "sentence_start_token = \"SENTENCE_START\"\n",
    "sentence_end_token = \"SENTENCE_END\"\n",
    "\n",
    "use_dropout = False\n",
    "num_epochs = 10\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "hidden_size = 128\n",
    "skip_window = 4  # How many words to consider left and right.\n",
    "num_skips = 2  # How many times to reuse an input to generate a label.\n",
    "num_sampled = 64\n",
    "num_steps = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  []\n",
    "for i in range(0, len(seinfeld_text_data)):\n",
    "    temp = seinfeld_text_data.iloc[i]['Dialogue']\n",
    "    if type(temp) is str:\n",
    "          \n",
    "        temp = nltk.sent_tokenize(temp)\n",
    "    \n",
    "        for j in range(0, len(temp)): \n",
    "            sentences.append(temp[j])\n",
    "   \n",
    "\n",
    "    #seinfeld_text_data.iloc[i]['Dialogue'] = temp\n",
    "    \n",
    "    \n",
    "    #seinfeld_text_data.iloc[i]['Dialogue'] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To be out, this is out...and out is one of the single most enjoyable experiences of life.'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sentences =  [temp_sent for sublist in tokenized_sentences for temp_sent in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = word_freq.most_common(VOCAB_SIZE-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_sentences = [w if w in word_to_index else unknown_token for w in flat_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.asarray([word_to_index[w] for w in flat_sentences[:600000]])\n",
    "valid_data = np.asarray([word_to_index[w] for w in flat_sentences[600000:800000]])\n",
    "test_data = np.asarray([word_to_index[w] for w in flat_sentences[800000:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  21,    1,  514,    3,   35,    1,    2,   15,   13,   87,   37,\n",
       "          4,   29,   60, 9999,    7, 9999, 9999, 7771,    0])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasBatchGenerator(object):\n",
    "\n",
    "    def __init__(self, data, num_steps, batch_size, vocabulary, skip_step=5):\n",
    "        self.data = data\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.vocabulary = vocabulary\n",
    "        # this will track the progress of the batches sequentially through the\n",
    "        # data set - once the data reaches the end of the data set it will reset\n",
    "        # back to zero\n",
    "        self.current_idx = 0\n",
    "        # skip_step is the number of words which will be skipped before the next\n",
    "        # batch is skimmed from the data set\n",
    "        self.skip_step = skip_step\n",
    "        \n",
    "    def generate(self):\n",
    "        x = np.zeros((self.batch_size, self.num_steps))\n",
    "        y = np.zeros((self.batch_size, self.num_steps, self.vocabulary))\n",
    "        while True:\n",
    "            for i in range(self.batch_size):\n",
    "                if self.current_idx + self.num_steps >= len(self.data):\n",
    "                # reset the index back to the start of the data set\n",
    "                    self.current_idx = 0\n",
    "                x[i, :] = self.data[self.current_idx:self.current_idx + self.num_steps]\n",
    "                temp_y = self.data[self.current_idx + 1:self.current_idx + self.num_steps + 1]\n",
    "                # convert all of temp_y into a one hot representation\n",
    "                y[i, :, :] = tf.keras.utils.to_categorical(temp_y, num_classes=self.vocabulary)\n",
    "                self.current_idx += self.skip_step\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = KerasBatchGenerator(train_data, num_steps, batch_size, VOCAB_SIZE,\n",
    "                                           skip_step=num_steps)\n",
    "valid_data_generator = KerasBatchGenerator(valid_data, num_steps, batch_size, VOCAB_SIZE,\n",
    "                                           skip_step=num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(VOCAB_SIZE, embedding_size, input_length=num_steps))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "model.add(tf.keras.layers.LSTM(hidden_size, return_sequences=True))\n",
    "if use_dropout:\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(VOCAB_SIZE)))\n",
    "model.add(tf.keras.layers.Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "390/390 [==============================] - 326s 836ms/step - loss: 6.3126 - categorical_accuracy: 0.0739 - val_loss: 6.1095 - val_categorical_accuracy: 0.0563\n",
      "Epoch 2/10\n",
      "390/390 [==============================] - 332s 851ms/step - loss: 6.0122 - categorical_accuracy: 0.0743 - val_loss: 6.1121 - val_categorical_accuracy: 0.0549\n",
      "Epoch 3/10\n",
      "390/390 [==============================] - 332s 850ms/step - loss: 6.0114 - categorical_accuracy: 0.0746 - val_loss: 6.1070 - val_categorical_accuracy: 0.0554\n",
      "Epoch 4/10\n",
      "390/390 [==============================] - 332s 852ms/step - loss: 6.0060 - categorical_accuracy: 0.0743 - val_loss: 6.1131 - val_categorical_accuracy: 0.0550\n",
      "Epoch 5/10\n",
      "390/390 [==============================] - 333s 853ms/step - loss: 6.0104 - categorical_accuracy: 0.0740 - val_loss: 6.0928 - val_categorical_accuracy: 0.0560\n",
      "Epoch 6/10\n",
      "390/390 [==============================] - 332s 852ms/step - loss: 6.0063 - categorical_accuracy: 0.0735 - val_loss: 6.1001 - val_categorical_accuracy: 0.0559\n",
      "Epoch 7/10\n",
      "390/390 [==============================] - 329s 844ms/step - loss: 6.0052 - categorical_accuracy: 0.0733 - val_loss: 6.0928 - val_categorical_accuracy: 0.0563\n",
      "Epoch 8/10\n",
      "390/390 [==============================] - 329s 844ms/step - loss: 6.0053 - categorical_accuracy: 0.0742 - val_loss: 6.1023 - val_categorical_accuracy: 0.0559\n",
      "Epoch 9/10\n",
      "390/390 [==============================] - 356s 914ms/step - loss: 6.0012 - categorical_accuracy: 0.0741 - val_loss: 6.1003 - val_categorical_accuracy: 0.0561\n",
      "Epoch 10/10\n",
      "390/390 [==============================] - 383s 982ms/step - loss: 6.0033 - categorical_accuracy: 0.0742 - val_loss: 6.0976 - val_categorical_accuracy: 0.0565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a34e5fef0>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(train_data_generator.generate(), len(train_data)//(batch_size*num_steps), num_epochs,\n",
    "                        validation_data=valid_data_generator.generate(),\n",
    "                        validation_steps=len(valid_data)//(batch_size*num_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:\n",
      "Actual words: This is what theyre talking about ... this whole thing \n",
      "Predicted words: , , , , , , , , , , \n"
     ]
    }
   ],
   "source": [
    "dummy_iters = 40\n",
    "example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, VOCAB_SIZE,\n",
    "                                                     skip_step=1)\n",
    "print(\"Training data:\")\n",
    "for i in range(dummy_iters):\n",
    "    dummy = next(example_training_generator.generate())\n",
    "num_predict = 10\n",
    "true_print_out = \"Actual words: \"\n",
    "pred_print_out = \"Predicted words: \"\n",
    "for i in range(num_predict):\n",
    "    data = next(example_training_generator.generate())\n",
    "    prediction = model.predict(data[0])\n",
    "    predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "    true_print_out += index_to_word[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "    pred_print_out += index_to_word[predict_word] + \" \"\n",
    "print(true_print_out)\n",
    "print(pred_print_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
